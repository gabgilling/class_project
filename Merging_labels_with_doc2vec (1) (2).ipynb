{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='18c9dc37-6f05-4132-a583-46842caa6329', project_access_token='p-2b4eb6562dcd081943e8d03040e939cec1c79724')\npc = project.project_context\n", "execution_count": 31, "outputs": []}, {"metadata": {"id": "i9dyAjrFrJUh", "colab_type": "text"}, "cell_type": "markdown", "source": "# A data-driven way to merge similar classes together\n\nJuan Lopez Martin"}, {"metadata": {"id": "fRD8tbDfrKe6", "colab_type": "text"}, "cell_type": "markdown", "source": "The quality of the classes is determinant for training a classifier. In the famous MNIST example, the handwritten number '6' was always labeled correclty as a '6'. However, it is clear it would be very problematic if that digit was sometimes labels as '6', others as 'six', 'SIX', '6s', etc. In this case, it would be necessary to merge all these labels together before training. If not, the classifier would probably pick up irrelevant features of the '6' to try to distinguish between the classes '6', 'six', '6s', etc and end up with much less prediction accuracy.\n\nWe are in a similar situation if we want to train a classifier to infer the issue from the narrative. For instance, some narratives have the issue 'Incorrect information on credit report', while others have 'Incorrect information on your report'. It is relatively clear that these two issues are not different, and that these two labels should be merged. The same happens, for instance, with 'Attempts to collect debt not owed' and 'Cont'd attempts collect debt not owed'."}, {"metadata": {"id": "ImhYK0F7trf2", "colab_type": "text"}, "cell_type": "markdown", "source": "## 0. Loading data"}, {"metadata": {}, "cell_type": "code", "source": "# Install gensim\n!pip install gensim", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Collecting gensim\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.2MB 12.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.12.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.2.0)\nCollecting smart-open>=1.8.1 (from gensim)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 28.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from gensim) (1.15.4)\nRequirement already satisfied: boto>=2.32 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\nRequirement already satisfied: boto3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.9.82)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\nRequirement already satisfied: botocore<1.13.0,>=1.12.82 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.82)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.3)\nRequirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.1.13)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.82->boto3->smart-open>=1.8.1->gensim) (0.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.82->boto3->smart-open>=1.8.1->gensim) (2.7.5)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\nSuccessfully built smart-open\nInstalling collected packages: smart-open, gensim\nSuccessfully installed gensim-3.8.1 smart-open-1.9.0\n", "name": "stdout"}]}, {"metadata": {"id": "8nNMnV8S21gS", "colab_type": "code", "outputId": "33c4ac62-4909-4dc6-8910-fd8896a4ff86", "executionInfo": {"status": "ok", "timestamp": 1571407729364, "user_tz": 240, "elapsed": 24459, "user": {"displayName": "Juan Lopez Martin", "photoUrl": "", "userId": "17739266498901215509"}}, "colab": {"base_uri": "https://localhost:8080/", "height": 125}}, "cell_type": "code", "source": "# Import packages for IBM Studio to load data\n\nimport types\nfrom botocore.client import Config\nimport ibm_boto3", "execution_count": 4, "outputs": []}, {"metadata": {"id": "7cxIonW52lKm", "colab_type": "code", "colab": {}}, "cell_type": "code", "source": "# Import packages\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport pickle\nfrom operator import itemgetter\nfrom itertools import compress\n#import spacy\n#import en_core_web_sm\n#nlp = en_core_web_sm.load()\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport networkx \nfrom networkx.algorithms.components.connected import connected_components\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load dataset\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_51cbd8ac4f124ba699737e1f03820b07 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='6FMtmurbcuYnZjqEkX-88omiJcLBrUuUuEmuWnVe5Erk',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_51cbd8ac4f124ba699737e1f03820b07.get_object(Bucket='qmssdseprojectfall2019-donotdelete-pr-ju1hanupbqko6m',Key='Consumer_Complaints.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body)\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (5,6,11,16) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "# Load pre-processed texts\n\nstreaming_body_2 = client_51cbd8ac4f124ba699737e1f03820b07.get_object(Bucket='qmssdseprojectfall2019-donotdelete-pr-ju1hanupbqko6m', Key='texts.pickle')['Body']\ntexts = pickle.loads(streaming_body_2.read())", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load pre-processed documents (same as texts with gensim format)\n\nstreaming_body_3 = client_51cbd8ac4f124ba699737e1f03820b07.get_object(Bucket='qmssdseprojectfall2019-donotdelete-pr-ju1hanupbqko6m', Key='documents.pickle')['Body']\ndocuments = pickle.loads(streaming_body_3.read())", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load trained doc2vec model\n\nstreaming_body_4 = client_51cbd8ac4f124ba699737e1f03820b07.get_object(Bucket='qmssdseprojectfall2019-donotdelete-pr-ju1hanupbqko6m', Key='model_100.pickle')['Body']\nmodel = pickle.loads(streaming_body_4.read())", "execution_count": 9, "outputs": []}, {"metadata": {"id": "6fJLU6IK2lLN", "colab_type": "code", "colab": {}}, "cell_type": "code", "source": "# Throw away rows without narrative\n\ndf = df.dropna(subset=['Consumer complaint narrative'])", "execution_count": 10, "outputs": []}, {"metadata": {"id": "GeFa8fafn1JH", "colab_type": "code", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "outputId": "1f828ca2-54f6-4210-c17c-50c05347d416", "executionInfo": {"status": "ok", "timestamp": 1571407951615, "user_tz": 240, "elapsed": 558, "user": {"displayName": "Juan Lopez Martin", "photoUrl": "", "userId": "17739266498901215509"}}}, "cell_type": "code", "source": "#This is to get a subset of the dataset. p=1 means we get 100% of the rows, \n# so in this case we are getting all of it. It is useful for exploring new \n# approaches.\n\np = 1\nrand = np.random.choice(a=[False, True], size=len(texts), p = [1-p, p])\n\ndf_s = df[rand]\n\ntexts_s = list(compress(texts, rand))\nlen(texts_s)\n\ndocuments_s = list(compress(documents, rand))\nlen(documents_s)", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "444683"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df_s.shape", "execution_count": 59, "outputs": [{"output_type": "execute_result", "execution_count": 59, "data": {"text/plain": "(444683, 18)"}, "metadata": {}}]}, {"metadata": {"id": "fyaBLYFy2lM1", "colab_type": "code", "outputId": "d811f26c-85a9-48e2-8dda-0daee3a9abc6", "executionInfo": {"status": "ok", "timestamp": 1571407959520, "user_tz": 240, "elapsed": 5677, "user": {"displayName": "Juan Lopez Martin", "photoUrl": "", "userId": "17739266498901215509"}}, "colab": {"base_uri": "https://localhost:8080/", "height": 34}}, "cell_type": "code", "source": "# We use L2 normalization on the vectors\nmodel.docvecs.init_sims(replace=True)\n\n# Then take the appropiate sample (again, by default we get the entire dataset)\nvecs_s = []\nfor i in range(0, df.shape[0]):\n  if rand[i] == True:\n    vecs_s.append(model.docvecs[i])\n    \nlen(vecs_s)", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "444683"}, "metadata": {}}]}, {"metadata": {"id": "5H5l9s8F2lLW", "colab_type": "text"}, "cell_type": "markdown", "source": "## 1. Data cleaning (skip, output pickled)"}, {"metadata": {"id": "WgDufrSnqL4_", "colab_type": "text"}, "cell_type": "markdown", "source": "We re using spacy to:\n\n* Remove the X, XX, XXX, etc.\n* Remove strings that are not alphanumeric\n* Remove spaces\n* Remove punctuation\n* Remove numbers\n* Lemmatize\n\nThis trasnforms the narrative to a list of words."}, {"metadata": {"id": "l2DjLNK82lL8", "colab_type": "code", "colab": {}}, "cell_type": "code", "source": "## Commented out -- output saved at model_100.pickle\n \n#xs = [\"X\"*2, \"X\"*3, \"X\"*4, \"X\"*5, \"X\"*6, \"X\"*7, \"X\"*8, \"X\"*9, \"X\"*10, \"X\"*11, \"XX/XX\", \"XX/XX/XXX\", \"XX/XX/XX\", \"XX/XXXX\"]\n\n#texts = []\n#for sent in nlp.pipe(df['Consumer complaint narrative'], disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"]):\n#    texts.append([word.lemma_ for word in sent \n#                  if word.is_alpha and\n#                  not word.is_space and\n#                  not word.is_stop and \n#                  not word.is_punct and \n#                  not word.like_num and \n#                     word.text not in xs])\n    \n#filename = folder+'texts.pickle'\n#outfile = open(filename,'wb')\n#pickle.dump(texts,outfile)\n#outfile.close()", "execution_count": 14, "outputs": []}, {"metadata": {"id": "5OOBLouX2lMR", "colab_type": "text"}, "cell_type": "markdown", "source": "## 2. Doc2vec (skip, output pickled)"}, {"metadata": {"id": "X3WUsvqEt7kC", "colab_type": "text"}, "cell_type": "markdown", "source": "Word embeddings are vector representations of individual words that carry their semantic meaning. If these word vectors are obtained from a large corpus we would expect, for instance, that the vector for frog and toad will be very similar. Additionally, these vectors capture more complex relationships. For instance, the association between 'Washington' and USA will be similar as the association between 'China' and 'Beijin'.\n\nDoc2vec is an extension to the famous word embedding approach that trains vector representations of entire documents instead of just words. If trained on a large set of books we would expect, for instance, that Statistics books will have similar vectors.\n\nIn this case, we will train document embeddings based on the narratives. In other words, each narrative will be transformed into a 300-dimensional vector. The main advantge of this approach over simpler methods is that it takes into account the order of the words in sentences. In fact, under the hood the vectors are built by trying to predict a center word based on the surrounding words (PV-DM). This allows to capture complex semantic relationships between words.\n\nThe hyperparamethers used here are relatively standard in the field:\n\n* 300-dimensional vectors.\n* Window of 15 words (meaning we try to predict the center word from a 15-word window).\n* Negative sampling of 5 -- improves results and reduces computational cost.\n* 100 epochs. This is good but it took 4 hours.\n"}, {"metadata": {"id": "0Fbp9RTzdUMS", "colab_type": "code", "colab": {}}, "cell_type": "code", "source": "## Commented out -- output saved at model_100.pickle\n\n# First, we create a taggeddocument object, necessary for word2vec. Then we\n# train the vecs.\n\n#documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]\n#model = Doc2Vec(documents, dm = 1, vector_size=300, window=15, negative=5, min_count=1, workers=4, epochs = 100)\n\n#filename = folder+'model_100.pickle'\n#outfile = open(filename,'wb')\n#model = pickle.dump(model, outfile)\n#outfile.close()", "execution_count": 15, "outputs": []}, {"metadata": {"id": "dpLwVyGOODYW", "colab_type": "text"}, "cell_type": "markdown", "source": "## 3. Labels to merge"}, {"metadata": {"id": "IZGd7vA0aafp", "colab_type": "text"}, "cell_type": "markdown", "source": "For each issue (class) we have a set of narratives (documents) that have been represented as (300-dimensional L2-normalized) vectors with the doc2vec algorithm. For example, for issue $A$ ('Incorrect information on your report') we have obtained $a_1, a_2, ..., a_{65508}$ vectors. One basic approach to summarize the information we have about the issue $A$ is to calculate the mean for all the vectors $a_i$. Doing this, we obtain an average (300-dimensional L2-normalized) vector that can be seen as an approximate representation of the issue $A$. The idea is that the average of the vectors $a_i$ should contain that is to a certain degree common for the issue $A$.   "}, {"metadata": {"id": "Ez8jqVbsIOXh", "colab_type": "code", "colab": {}}, "cell_type": "code", "source": "# Create list of issues ordered by value_counts\nissues_list = list(df_s['Issue'].value_counts().index)\n\n# Calculate the mean for the vectors corresponding to each issue\nvec_list = [np.mean(np.array(list(compress(vecs_s, list(df[rand]['Issue']==issue)))), axis  = 0) for issue in issues_list]", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# vecs_s is list of vectorized narratives for the whole dataset (len 444683)", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "444683"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### a. First Round of Clustering/issue reduction (using original issue labels)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now we have a vector representation of issue $A$, $B$, $C$, etc. In word2vec the similarity of two vectors is measured using cosine similarity, which determines whether the two vectors are pointing in approximately the same direction.\n\n\nWe will create a similarity matrix that reports the cosine similarity between all the issues. If you have no experience with word2vec, this can be conceptually similar to a correlation matrix. That is, issues that are similar between each other will have a cosine similarity close to one and non-related issues a score close to zero."}, {"metadata": {}, "cell_type": "code", "source": "# Create distance matrix with cosine similarity\ndist = 1-pairwise_distances(np.array(vec_list), metric=\"cosine\")\n\n# Put the distance matrix in a dataframe with index and column names\ndist_df = pd.DataFrame(dist, index = issues_list, columns=issues_list)\n\ndist_df.iloc[0:5,0:5]", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "                                                    Incorrect information on your report  \\\nIncorrect information on your report                                            1.000000   \nProblem with a credit reporting company's inves...                              0.988587   \nAttempts to collect debt not owed                                               0.960438   \nIncorrect information on credit report                                          0.990585   \nImproper use of your report                                                     0.962792   \n\n                                                    Problem with a credit reporting company's investigation into an existing problem  \\\nIncorrect information on your report                                                         0.988587                                  \nProblem with a credit reporting company's inves...                                           1.000000                                  \nAttempts to collect debt not owed                                                            0.947416                                  \nIncorrect information on credit report                                                       0.988502                                  \nImproper use of your report                                                                  0.950098                                  \n\n                                                    Attempts to collect debt not owed  \\\nIncorrect information on your report                                         0.960438   \nProblem with a credit reporting company's inves...                           0.947416   \nAttempts to collect debt not owed                                            1.000000   \nIncorrect information on credit report                                       0.947011   \nImproper use of your report                                                  0.929568   \n\n                                                    Incorrect information on credit report  \\\nIncorrect information on your report                                              0.990585   \nProblem with a credit reporting company's inves...                                0.988502   \nAttempts to collect debt not owed                                                 0.947011   \nIncorrect information on credit report                                            1.000000   \nImproper use of your report                                                       0.955511   \n\n                                                    Improper use of your report  \nIncorrect information on your report                                   0.962792  \nProblem with a credit reporting company's inves...                     0.950098  \nAttempts to collect debt not owed                                      0.929568  \nIncorrect information on credit report                                 0.955511  \nImproper use of your report                                            1.000000  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Incorrect information on your report</th>\n      <th>Problem with a credit reporting company's investigation into an existing problem</th>\n      <th>Attempts to collect debt not owed</th>\n      <th>Incorrect information on credit report</th>\n      <th>Improper use of your report</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Incorrect information on your report</th>\n      <td>1.000000</td>\n      <td>0.988587</td>\n      <td>0.960438</td>\n      <td>0.990585</td>\n      <td>0.962792</td>\n    </tr>\n    <tr>\n      <th>Problem with a credit reporting company's investigation into an existing problem</th>\n      <td>0.988587</td>\n      <td>1.000000</td>\n      <td>0.947416</td>\n      <td>0.988502</td>\n      <td>0.950098</td>\n    </tr>\n    <tr>\n      <th>Attempts to collect debt not owed</th>\n      <td>0.960438</td>\n      <td>0.947416</td>\n      <td>1.000000</td>\n      <td>0.947011</td>\n      <td>0.929568</td>\n    </tr>\n    <tr>\n      <th>Incorrect information on credit report</th>\n      <td>0.990585</td>\n      <td>0.988502</td>\n      <td>0.947011</td>\n      <td>1.000000</td>\n      <td>0.955511</td>\n    </tr>\n    <tr>\n      <th>Improper use of your report</th>\n      <td>0.962792</td>\n      <td>0.950098</td>\n      <td>0.929568</td>\n      <td>0.955511</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "We have to set up a threshold to decide which issues to merge and which issues to keep separate. In this case I have decided for 0.985, and in my experience something between 0.98 and 0.99 seems like a reasonable threshold. However, we can discuss the appropiate threshold. \n\n\nNote that merges of  issues can be multiple. Namely, if issue $A$ is going to be merged with $B$ and $B$ is also going to be merged with $C$, we are going to obtain one new issue $K = A \\cup B \\cup C$. This is similar to say that, given a network $G = (V, E)$ in which each vertex is an original issue and the edges are $1$ if the issues are going to be merged and $0$ otherwise, we are going to keep all the distinct components as labels."}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "# The issues to merge are decided according to an user-defined threshold.\nthreshold = 0.985\nl = [list(dist_df[dist_df.loc[issue]>threshold].index) for issue in issues_list]\n\n# Finding components in the network.\n# Code from https://stackoverflow.com/questions/4842613/merge-lists-that-share-common-elements?lq=1\n\ndef to_graph(l):\n    G = networkx.Graph()\n    for part in l:\n        # each sublist is a bunch of nodes\n        G.add_nodes_from(part)\n        # it also imlies a number of edges:\n        G.add_edges_from(to_edges(part))\n    return G\n\ndef to_edges(l):\n    \"\"\" \n        treat `l` as a Graph and returns it's edges \n        to_edges(['a','b','c','d']) -> [(a,b), (b,c),(c,d)]\n    \"\"\"\n    it = iter(l)\n    last = next(it)\n\n    for current in it:\n        yield last, current\n        last = current    \n\nG = to_graph(l)\ncomponents = list(connected_components(G))\n\nfor aset in components:\n  if len(aset)>1:\n    for element in aset:\n      print(element)\n    print(\"\\n\")", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Credit reporting company's investigation\nIncorrect information on your report\nIncorrect information on credit report\nProblem with a credit reporting company's investigation into an existing problem\n\n\nCont'd attempts collect debt not owed\nAttempts to collect debt not owed\n\n\nTrouble during payment process\nLoan servicing, payments, escrow account\n\n\nDisclosure verification of debt\nWritten notification about debt\n\n\nStruggling to pay mortgage\nLoan modification,collection,foreclosure\n\n\nDeposits and withdrawals\nManaging an account\n\n\nTook or threatened to take negative or legal action\nTaking/threatening an illegal action\n\n\nApplication, originator, mortgage broker\nApplying for a mortgage or refinancing an existing mortgage\n\n\nCan't repay my loan\nStruggling to repay your loan\n\n\nUnable to get your credit report or credit score\nUnable to get credit report/credit score\n\n\nProblems caused by my funds being low\nProblem caused by your funds being low\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df['cluster_1'] = np.nan", "execution_count": 17, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "print(len(components)) # we've boiled down 166 issues to 148 as part of the first round of clustering\n# now we want to give each of these clusters a number ID, and assign them where appropriate in the dataframe\n\nfor i, comp in enumerate(components):\n    df['cluster_1'].loc[df['Issue'].isin(list(comp))] = i\n    print(i, comp)", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "148\n", "name": "stdout"}, {"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n", "name": "stderr"}, {"output_type": "stream", "text": "0 {\"Credit reporting company's investigation\", 'Incorrect information on your report', 'Incorrect information on credit report', \"Problem with a credit reporting company's investigation into an existing problem\"}\n1 {\"Cont'd attempts collect debt not owed\", 'Attempts to collect debt not owed'}\n2 {'Improper use of your report'}\n3 {'Trouble during payment process', 'Loan servicing, payments, escrow account'}\n4 {'Communication tactics'}\n5 {'Disclosure verification of debt', 'Written notification about debt'}\n6 {'Struggling to pay mortgage', 'Loan modification,collection,foreclosure'}\n7 {'False statements or representation'}\n8 {'Deposits and withdrawals', 'Managing an account'}\n9 {'Dealing with your lender or servicer'}\n10 {'Dealing with my lender or servicer'}\n11 {'Managing the loan or lease'}\n12 {'Took or threatened to take negative or legal action', 'Taking/threatening an illegal action'}\n13 {'Account opening, closing, or management'}\n14 {'Problem with a purchase shown on your statement'}\n15 {'Other features, terms, or problems'}\n16 {'Application, originator, mortgage broker', 'Applying for a mortgage or refinancing an existing mortgage'}\n17 {'Fees or interest'}\n18 {'Problem when making payments'}\n19 {\"Can't repay my loan\", 'Struggling to repay your loan'}\n20 {'Fraud or scam'}\n21 {'Billing disputes'}\n22 {'Improper contact or sharing of info'}\n23 {'Problem with fraud alerts or security freezes'}\n24 {'Unable to get your credit report or credit score', 'Unable to get credit report/credit score'}\n25 {'Problems when you are unable to pay'}\n26 {'Closing on a mortgage'}\n27 {'Struggling to pay your loan'}\n28 {'Settlement process and costs'}\n29 {'Getting a credit card'}\n30 {'Closing your account'}\n31 {'Advertising and marketing, including promotional offers'}\n32 {'Closing an account'}\n33 {'Problems caused by my funds being low', 'Problem caused by your funds being low'}\n34 {'Credit monitoring or identity theft protection services'}\n35 {'Other'}\n36 {'Opening an account'}\n37 {'Problem with a lender or other company charging your account'}\n38 {'Threatened to contact someone or share information improperly'}\n39 {'Identity theft / Fraud / Embezzlement'}\n40 {'Using a debit or ATM card'}\n41 {\"Charged fees or interest you didn't expect\"}\n42 {'Taking out the loan or lease'}\n43 {'Money was not available when promised'}\n44 {'Improper use of my credit report'}\n45 {'Closing/Cancelling account'}\n46 {'Making/receiving payments, sending money'}\n47 {'Other transaction problem'}\n48 {'Problems at the end of the loan or lease'}\n49 {'Credit decision / Underwriting'}\n50 {'Trouble using your card'}\n51 {'Customer service / Customer relations'}\n52 {'Credit monitoring or identity protection'}\n53 {'Rewards'}\n54 {\"Charged fees or interest I didn't expect\"}\n55 {'Delinquent account'}\n56 {'Getting a loan or lease'}\n57 {'Advertising and marketing'}\n58 {'APR or interest rate'}\n59 {'Late fee'}\n60 {'Credit card protection / Debt protection'}\n61 {'Transaction issue'}\n62 {'Shopping for a loan or lease'}\n63 {'Billing statement'}\n64 {'Unexpected or other fees'}\n65 {'Problem with a purchase or transfer'}\n66 {'Problem with the payoff process at the end of the loan'}\n67 {'Unauthorized transactions/trans. issues'}\n68 {'Payoff process'}\n69 {'Credit determination'}\n70 {'Other fee'}\n71 {'Unsolicited issuance of credit card'}\n72 {'Struggling to pay your bill'}\n73 {'Other transaction issues'}\n74 {'Unauthorized transactions or other transaction problem'}\n75 {'Credit line increase/decrease'}\n76 {'Getting a loan'}\n77 {'Managing, opening, or closing your mobile wallet account'}\n78 {\"Problem with a company's investigation into an existing issue\"}\n79 {'Managing, opening, or closing account'}\n80 {\"Can't contact lender\"}\n81 {'Other service problem'}\n82 {'Trouble using the card'}\n83 {'Getting the loan'}\n84 {'Balance transfer'}\n85 {'Problem with customer service'}\n86 {'Confusing or missing disclosures'}\n87 {'Identity theft protection or other monitoring services'}\n88 {'Problem getting a card or closing an account'}\n89 {'Wrong amount charged or received'}\n90 {\"Received a loan I didn't apply for\"}\n91 {'Applying for a mortgage'}\n92 {\"Can't stop charges to bank account\"}\n93 {'Confusing or misleading advertising or marketing'}\n94 {'Getting a line of credit'}\n95 {'Problem with additional add-on products or services'}\n96 {\"Can't contact lender or servicer\"}\n97 {'Payment to acct not credited'}\n98 {'Other service issues'}\n99 {\"Received a loan you didn't apply for\"}\n100 {\"Can't stop withdrawals from your bank account\"}\n101 {\"Loan payment wasn't credited to your account\"}\n102 {'Application processing delay'}\n103 {'Charged bank acct wrong day or amt'}\n104 {'Privacy'}\n105 {'Sale of account'}\n106 {'Bankruptcy'}\n107 {'Adding money'}\n108 {'Forbearance / Workout plans'}\n109 {'Applied for loan/did not receive money'}\n110 {'Vehicle was repossessed or sold the vehicle'}\n111 {'Arbitration'}\n112 {'Problem adding money'}\n113 {'Fees'}\n114 {'Money was taken from your bank account on the wrong day or for the wrong amount'}\n115 {'Excessive fees'}\n116 {'Advertising'}\n117 {\"Was approved for a loan, but didn't receive the money\"}\n118 {'Incorrect/missing disclosures or info'}\n119 {'Lost or stolen check'}\n120 {'Customer service/Customer relations'}\n121 {'Lost or stolen money order'}\n122 {'Balance transfer fee'}\n123 {'Cash advance'}\n124 {'Cash advance fee'}\n125 {'Convenience checks'}\n126 {'Vehicle was damaged or destroyed the vehicle'}\n127 {'Overlimit fee'}\n128 {'Lender repossessed or sold the vehicle'}\n129 {'Advertising, marketing or disclosures'}\n130 {'Credit limit changed'}\n131 {'Incorrect exchange rate'}\n132 {'Unexpected/Other fees'}\n133 {'Problem with cash advance'}\n134 {'Overdraft, savings or rewards features'}\n135 {'Account terms and changes'}\n136 {'Managing the line of credit'}\n137 {'Problem with overdraft'}\n138 {'Disclosures'}\n139 {\"Was approved for a loan, but didn't receive money\"}\n140 {'Overdraft, savings, or rewards features'}\n141 {'Shopping for a line of credit'}\n142 {'Property was sold'}\n143 {'Lender sold the property'}\n144 {'Lender damaged or destroyed vehicle'}\n145 {'Problem with an overdraft'}\n146 {'Property was damaged or destroyed property'}\n147 {'Lender damaged or destroyed property'}\n", "name": "stdout"}]}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "#### b. Repeat steps twice // we could write a for loop that takes care of this for us but now stick to mechanical code"}, {"metadata": {}, "cell_type": "code", "source": "# Create list of issues ordered by value_counts\nissues_list = list(df['cluster_1'].value_counts().index)\n\n# Calculate the mean for the vectors corresponding to each issue\nvec_list = [np.mean(np.array(list(compress(vecs_s, list(df[rand]['cluster_1']==issue)))), axis  = 0) for issue in issues_list]", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dist = 1-pairwise_distances(np.array(vec_list), metric=\"cosine\")\n\n# Put the distance matrix in a dataframe with index and column names\ndist_df = pd.DataFrame(dist, index = issues_list, columns=issues_list)", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dist_df.iloc[0:5,0:5] # notice how similarities have dropped, this is normal because more different narratives are clustered together\n# what should we pick for the new threshold? we'll pick 0.9 to start with", "execution_count": 21, "outputs": [{"output_type": "execute_result", "execution_count": 21, "data": {"text/plain": "          0.0       1.0       3.0       5.0       2.0\n0.0  1.000000  0.957306  0.854775  0.948433  0.961159\n1.0  0.957306  1.000000  0.835960  0.983153  0.929901\n3.0  0.854775  0.835960  1.000000  0.823749  0.848809\n5.0  0.948433  0.983153  0.823749  1.000000  0.925640\n2.0  0.961159  0.929901  0.848809  0.925640  1.000000", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.0</th>\n      <th>1.0</th>\n      <th>3.0</th>\n      <th>5.0</th>\n      <th>2.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>1.000000</td>\n      <td>0.957306</td>\n      <td>0.854775</td>\n      <td>0.948433</td>\n      <td>0.961159</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>0.957306</td>\n      <td>1.000000</td>\n      <td>0.835960</td>\n      <td>0.983153</td>\n      <td>0.929901</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>0.854775</td>\n      <td>0.835960</td>\n      <td>1.000000</td>\n      <td>0.823749</td>\n      <td>0.848809</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>0.948433</td>\n      <td>0.983153</td>\n      <td>0.823749</td>\n      <td>1.000000</td>\n      <td>0.925640</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>0.961159</td>\n      <td>0.929901</td>\n      <td>0.848809</td>\n      <td>0.925640</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "threshold = 0.95 # lower than 0.95 yields unreasonably big clusters\nl = [list(dist_df[dist_df.loc[issue]>threshold].index) for issue in issues_list]\n\nG = to_graph(l)\ncomponents = list(connected_components(G))\n\nfor aset in components:\n    if len(aset)>1:\n        for element in aset:\n              print(element)\n        print(\"\\n\")", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "0.0\n1.0\n2.0\n4.0\n5.0\n38.0\n7.0\n44.0\n12.0\n22.0\n\n\n8.0\n40.0\n37.0\n46.0\n\n\n9.0\n10.0\n19.0\n\n\n42.0\n11.0\n56.0\n25.0\n27.0\n62.0\n\n\n16.0\n49.0\n91.0\n\n\n35.0\n14.0\n15.0\n51.0\n21.0\n55.0\n60.0\n61.0\n\n\n17.0\n70.0\n\n\n34.0\n52.0\n87.0\n23.0\n\n\n26.0\n28.0\n\n\n69.0\n29.0\n\n\n45.0\n30.0\n\n\n57.0\n31.0\n\n\n41.0\n66.0\n54.0\n\n\n73.0\n43.0\n47.0\n\n\n65.0\n67.0\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df['cluster_2'] = np.nan", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(components)", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "107"}, "metadata": {}}]}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "for i, comp in enumerate(components):\n    df['cluster_2'].loc[df['cluster_1'].isin(list(comp))] = i\n    print(i, comp)", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n", "name": "stderr"}, {"output_type": "stream", "text": "0 {0.0, 1.0, 2.0, 4.0, 5.0, 38.0, 7.0, 44.0, 12.0, 22.0}\n1 {3.0}\n2 {6.0}\n3 {8.0, 40.0, 37.0, 46.0}\n4 {9.0, 10.0, 19.0}\n5 {42.0, 11.0, 56.0, 25.0, 27.0, 62.0}\n6 {13.0}\n7 {16.0, 49.0, 91.0}\n8 {35.0, 14.0, 15.0, 51.0, 21.0, 55.0, 60.0, 61.0}\n9 {24.0}\n10 {17.0, 70.0}\n11 {33.0}\n12 {18.0}\n13 {20.0}\n14 {34.0, 52.0, 87.0, 23.0}\n15 {26.0, 28.0}\n16 {69.0, 29.0}\n17 {45.0, 30.0}\n18 {57.0, 31.0}\n19 {32.0}\n20 {36.0}\n21 {39.0}\n22 {41.0, 66.0, 54.0}\n23 {73.0, 43.0, 47.0}\n24 {48.0}\n25 {50.0}\n26 {53.0}\n27 {58.0}\n28 {59.0}\n29 {63.0}\n30 {64.0}\n31 {65.0, 67.0}\n32 {68.0}\n33 {71.0}\n34 {72.0}\n35 {74.0}\n36 {75.0}\n37 {76.0}\n38 {77.0}\n39 {78.0}\n40 {79.0}\n41 {80.0}\n42 {81.0}\n43 {82.0}\n44 {83.0}\n45 {84.0}\n46 {85.0}\n47 {86.0}\n48 {88.0}\n49 {89.0}\n50 {90.0}\n51 {92.0}\n52 {93.0}\n53 {94.0}\n54 {95.0}\n55 {97.0}\n56 {96.0}\n57 {98.0}\n58 {99.0}\n59 {100.0}\n60 {101.0}\n61 {102.0}\n62 {103.0}\n63 {104.0}\n64 {105.0}\n65 {106.0}\n66 {107.0}\n67 {108.0}\n68 {109.0}\n69 {110.0}\n70 {111.0}\n71 {112.0}\n72 {113.0}\n73 {115.0}\n74 {114.0}\n75 {116.0}\n76 {117.0}\n77 {118.0}\n78 {119.0}\n79 {120.0}\n80 {121.0}\n81 {122.0}\n82 {123.0}\n83 {124.0}\n84 {125.0}\n85 {127.0}\n86 {126.0}\n87 {128.0}\n88 {130.0}\n89 {129.0}\n90 {131.0}\n91 {132.0}\n92 {133.0}\n93 {135.0}\n94 {134.0}\n95 {136.0}\n96 {138.0}\n97 {137.0}\n98 {139.0}\n99 {140.0}\n100 {141.0}\n101 {142.0}\n102 {143.0}\n103 {144.0}\n104 {145.0}\n105 {146.0}\n106 {147.0}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "issues_list = list(df['cluster_2'].value_counts().index)\n\n# Calculate the mean for the vectors corresponding to each issue\nvec_list = [np.mean(np.array(list(compress(vecs_s, list(df[rand]['cluster_2']==issue)))), axis  = 0) for issue in issues_list]", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dist = 1-pairwise_distances(np.array(vec_list), metric=\"cosine\")\n\n# Put the distance matrix in a dataframe with index and column names\ndist_df = pd.DataFrame(dist, index = issues_list, columns=issues_list)\ndist_df.iloc[0:5,0:5]", "execution_count": 27, "outputs": [{"output_type": "execute_result", "execution_count": 27, "data": {"text/plain": "          0.0       1.0       4.0       2.0       8.0\n0.0  1.000000  0.860468  0.849357  0.825983  0.903086\n1.0  0.860468  1.000000  0.776079  0.891490  0.772692\n4.0  0.849357  0.776079  1.000000  0.755381  0.760120\n2.0  0.825983  0.891490  0.755381  1.000000  0.734871\n8.0  0.903086  0.772692  0.760120  0.734871  1.000000", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.0</th>\n      <th>1.0</th>\n      <th>4.0</th>\n      <th>2.0</th>\n      <th>8.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>1.000000</td>\n      <td>0.860468</td>\n      <td>0.849357</td>\n      <td>0.825983</td>\n      <td>0.903086</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>0.860468</td>\n      <td>1.000000</td>\n      <td>0.776079</td>\n      <td>0.891490</td>\n      <td>0.772692</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>0.849357</td>\n      <td>0.776079</td>\n      <td>1.000000</td>\n      <td>0.755381</td>\n      <td>0.760120</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>0.825983</td>\n      <td>0.891490</td>\n      <td>0.755381</td>\n      <td>1.000000</td>\n      <td>0.734871</td>\n    </tr>\n    <tr>\n      <th>8.0</th>\n      <td>0.903086</td>\n      <td>0.772692</td>\n      <td>0.760120</td>\n      <td>0.734871</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "threshold = 0.92 \nl = [list(dist_df[dist_df.loc[issue]>threshold].index) for issue in issues_list]\n\nG = to_graph(l)\ncomponents = list(connected_components(G))\n\nprint(len(components))\n\nfor aset in components:\n    if len(aset)>1:\n        for element in aset:\n              print(element)\n        print(\"\\n\")", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "63\n0.0\n32.0\n33.0\n70.0\n39.0\n8.0\n9.0\n10.0\n12.0\n14.0\n46.0\n16.0\n61.0\n52.0\n21.0\n27.0\n28.0\n29.0\n\n\n4.0\n37.0\n\n\n19.0\n3.0\n20.0\n6.0\n\n\n24.0\n5.0\n\n\n15.0\n7.0\n\n\n35.0\n38.0\n71.0\n42.0\n13.0\n47.0\n49.0\n23.0\n\n\n41.0\n50.0\n22.0\n55.0\n56.0\n58.0\n60.0\n\n\n18.0\n26.0\n\n\n40.0\n43.0\n48.0\n30.0\n31.0\n\n\n25.0\n36.0\n\n\n44.0\n53.0\n\n\n59.0\n51.0\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df['cluster_3'] = np.nan", "execution_count": 29, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "for i, comp in enumerate(components):\n    df['cluster_3'].loc[df['cluster_2'].isin(list(comp))] = i\n    print(i, comp)", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n", "name": "stderr"}, {"output_type": "stream", "text": "0 {0.0, 32.0, 33.0, 70.0, 39.0, 8.0, 9.0, 10.0, 12.0, 14.0, 46.0, 16.0, 61.0, 52.0, 21.0, 27.0, 28.0, 29.0}\n1 {1.0}\n2 {4.0, 37.0}\n3 {2.0}\n4 {19.0, 3.0, 20.0, 6.0}\n5 {24.0, 5.0}\n6 {15.0, 7.0}\n7 {17.0}\n8 {11.0}\n9 {35.0, 38.0, 71.0, 42.0, 13.0, 47.0, 49.0, 23.0}\n10 {41.0, 50.0, 22.0, 55.0, 56.0, 58.0, 60.0}\n11 {18.0, 26.0}\n12 {40.0, 43.0, 48.0, 30.0, 31.0}\n13 {25.0, 36.0}\n14 {34.0}\n15 {44.0, 53.0}\n16 {45.0}\n17 {59.0, 51.0}\n18 {54.0}\n19 {57.0}\n20 {62.0}\n21 {63.0}\n22 {64.0}\n23 {65.0}\n24 {66.0}\n25 {67.0}\n26 {68.0}\n27 {69.0}\n28 {72.0}\n29 {73.0}\n30 {74.0}\n31 {75.0}\n32 {76.0}\n33 {77.0}\n34 {78.0}\n35 {79.0}\n36 {81.0}\n37 {80.0}\n38 {82.0}\n39 {83.0}\n40 {84.0}\n41 {85.0}\n42 {87.0}\n43 {86.0}\n44 {88.0}\n45 {89.0}\n46 {90.0}\n47 {91.0}\n48 {92.0}\n49 {94.0}\n50 {93.0}\n51 {95.0}\n52 {96.0}\n53 {97.0}\n54 {98.0}\n55 {99.0}\n56 {100.0}\n57 {101.0}\n58 {102.0}\n59 {104.0}\n60 {103.0}\n61 {105.0}\n62 {106.0}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "len(df['cluster_3'].unique())", "execution_count": 36, "outputs": [{"output_type": "execute_result", "execution_count": 36, "data": {"text/plain": "63"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## We've reduced the number of clusters from 166 to 63!"}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(\"df_clustered.csv\", df.to_csv())", "execution_count": 32, "outputs": [{"output_type": "execute_result", "execution_count": 32, "data": {"text/plain": "{'file_name': 'df_clustered.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'qmssdseprojectfall2019-donotdelete-pr-ju1hanupbqko6m',\n 'asset_id': 'b24b5117-c47f-4d94-ae30-125dae62d3fa'}"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "colab": {"name": "Merging_labels_with_doc2vec.ipynb", "provenance": [], "collapsed_sections": []}}, "nbformat": 4, "nbformat_minor": 1}